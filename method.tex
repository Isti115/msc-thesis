\chapter{Method}
\label{chp:method}

The development process was done in an iterative style, meaning that we had to backtrack multiple times and rewrite certain parts over and over, but this often helped in finding better solutions. We believe that experimenting with multiple variations instead of accepting the first one that works is vital for converging to a better result instead of achieving a certain goal earlier only to move on. This process can lead to code that is more expressive, concise, easier to understand and thus maintain and explain, reason with and about.

This approach also results in the later layers built on top of earlier parts having a better foundation. The complexity of the definitions of data types and the functions describing their behaviors directly impacts the complexity of the proofs written for statements about them. Finding a simpler, but equivalent way to formalize a definition, that still results in equivalent semantics can lead to greatly simpler proofs.

After formalizing certain constructs and writing some proofs in connection with them it is usually much easier to see what causes complications and how to modify them to make their handling easier. These discoveries have sometimes led to decisions that diverged from the original material, warranted by the differences in required precision between paper based proofs, where certain parts can be omitted and computer based proofs, that need every little detail to be verified.

Sometimes the formalization of certain parts led to deeper understanding of their behavior, because of which we were able to restructure the definitions and proofs in a way that helped incorporate more and more elements from the Agda Standard Library\cite{agda-stdlib}, thus reducing the amount of added code and representing parts of our formalization as specialized versions of more generic concepts.

The work was done in smaller segments, which made it feasible to build and extend the model step-by-step instead of having to write one huge monolithic codebase all at once without being able to test smaller parts of it. This approach sometimes had drawbacks, when adding a new element required deep modifications in the existing parts, but overall it helped in advancing the project at a steady pace.

We started with an implementation of a highly simplified model in Idris, that served as an initial proof-of-concept. The main outlines were laid down and valuable experience was gained from discovering different possible workflows for the transformation of paper based mathematical definitions into code. Some simple proofs were written using that, but after trying to expand to a more complex model, the type checking of Idris seemed to be too slow, which led to the decision of switching to Agda.

Luckily the similarities of Agda and Idris meant, that our previous work was not lost, because it was easily translatable into the new environment. Parallel to the development of the base model we started including some statements in our code as well, which we prove for the current state of the codebase, and seeing if they were still true after modifications to the underlying model helped us determine if the introduction of any given change would break the consistency of the model or not.

% With helyett segédfüggvény -> könnyebb bizonyítani
After proofs started getting longer and more complicated, we started looking into ways to simplify and shorten them. For example we were able to achieve cleaner types during the interactive hole based proof process after eliminating dependent branching from them by rewriting some semantic functions to use auxiliary functions instead of using the built-in with-abstraction provided by Agda.

% Listák függvényként ábrázolása -> kiértékeli + rövidebb
We managed to further simplify proofs by moving the representation of arrays from lists to functions, which map from the set of natural numbers to the type of the contents of the array. By introducing this Agda could rely on the fact that functions always evaluate and simplify the proofs by computing the required types automatically, instead of us having to do pattern matches on the constructors of lists, thus several proofs became much shorter.

% \bN helyett Fin ("Az állapottér véges sok legfeljebb megszámlálhatóan végtelen típusértékhalmaz direktszorzata [Fót 83].")
The iterative method also applied to our aim of increasing the precision of the formalization in terms of similarity to the original material where it was possible. Sometimes it was easier to experiment with simplified versions first, which then could be later adapted in their details to be more consistent with the subject. An example for such is the moving of variable indexes to finite sets instead of the initially introduced state space that was mapping from natural numbers. This was done as a step towards conformance with the statement defining state space as a Cartesian product of finitely many type value sets. We generally tried to stay as close as possible, but we encountered some parts, which required modifications, because they were designed to be convenient for human reasoning, but are not formal enough to be sufficient for type checking.

% Assertionben implementált külön-külön \lfloor és \rfloor helyett Decision bevezetése -> minden sokkal könyebb / rövidebb
Another philosophy, that we tried to follow was the reduction of duplication by extracting and reusing code that appeared in multiple places. One example for this was the handling of Predicates, Conditions and Assertions. Originally, we had the \textbf{Predicate} inductive datatype, which described certain constraints that could either be checked if they evaluate to true in a certain state by converting them to a \textbf{Condition}, or a proof could be given that claims that they are satisfied by converting them to an \textbf{Assertion}. (More details about this will be covered in Chapter \ref{chp:results}.) These conversions had common parts which we later extracted into an intermediary representation by introducing the \textbf{Decision} type, which unified the shared part of the semantics and could later be turned into either a \textbf{Condition} or an \textbf{Assertion} more easily.

\newpage

A similar change was introduced, when we split the definition of \textbf{Ensures} ($\mapsto$) by introducing \textbf{Progress} ($\rightarrowtail$), which allowed us to construct partial proofs that applied to \textbf{Progress} on its own and later utilize them as parts of the proofs for \textbf{Ensures}. This resulted in a better structured and more readable code.

When introducing a new construct, it was often implemented in a very specific way at first, which aided the initial implementation, but after gaining a better understanding of the behavior and meaning of that certain part, by for example writing some proofs for it, they were often rewritten in a more generic fashion in order to make the model as extensible as possible, for example only introducing arrays of natural numbers first and later extending the array type to be parametrized with the specific set of types defined.

As another example for this, adding general equality of expressions as a predicate instead of only comparing natural numbers required the introduction of the notion of our own equality, since decidable equality for functions cannot be implemented, and that was needed due to the conversion of predicates to conditions. (This would have been necessary anyway when introducing data channels, since their equality is defined as a quotient with regards to their actual contents, their history does not matter during comparisons.) This generic decidable equality made the storage of array lengths necessary as well, since the comparison function needed to know which part of the function's domain it has to evaluate. When we replaced the built in equality with our own alternative, we used a function at first, that mapped to a different type for each datatype constructor (for example it simply mapped back to the built-in equality for natural numbers), but that turned many implicit variables that were previously automatically solved in our proofs unsolvable by Agda, thus we changed the representation to an inductive datatype with a single constructor instead of the function and successfully aided Agda in solving the metavariables this way.

% ownEq -> ownDecEq -> Arrays need length
% ownEq as datatype -> Statements work again without implicits

If we had tried to do all of these changes at once, it would have been easy to get lost dealing with multiple design decisions at the same time and not really seeing a clear way forward, but dealing with them step by step allowed us to keep the tasks in a visible scope and not lose track of our goals.

% s0 implicit eltűnik, mert nincs rá szükség -> nem tudja kitalálni az Agda
After formalizing statements that describe some safety and progress properties of programs we noticed that there was a certain implicit variable that Agda had trouble figuring out on its own in multiple scenarios. This was the $s_0$ initialization instruction, the first part of the representation of a parallel program, which the original material defines as a pair made out of a single initialization instruction and a set of conditional instructions that can be executed afterwards in any order. By observing the definitions we realized that it is indeed essentially irrelevant at the later stages of the program and only plays a role in constructions dealing with the beginning, such as \textbf{INIT} and \textbf{inv}. By splitting up the definition into the separate notions of \textbf{ParallelProgram} and \textbf{InitializedParallelProgram}, we were able to avoid the extraneous inclusion in the properties for which it was unnecessary and eliminate the ambiguous implicit variables.

As the credibility of the results that are proven in such a system depend greatly on the soundness of the underlying implementation, we have also added some safeguards in forms of assertions that describe the desired behavior of the implemented semantic functions. While this does not fully cover all the possible mistakes, by constructing instances for these types, we confirmed that some important properties hold for the foundation upon which our further work is based.

Finally, we have prepared the system for the inclusion of data channels, by implementing a type called \textbf{QueueWithHistory} and the functions that are necessary for handling the manipulation of its representation. The particularity of channels in the sense, that they act more like objects with methods makes the description of their behavior with just simple assignments complicated, which is undesirable, since it would greatly increase the complexity of not only their definitions which in turn also makes it more prone to mistakes, but also undermine the clarity of proofs based around them. Because of this, we had to change the representation of instructions from a condition paired with a list of variables with value expressions to be assigned. Instead we now have a condition paired with a list of instructions, that will include the extension (enqueue) and removal (dequeue) operations for channels. The problem is that channels would need to be excluded from assignments, so their values cannot be arbitrarily overwritten. Due to time constraints we were not able to devise a clean method that achieves the correct behavior, thus proofs about channels are outside of the scope of this current paper.

% TODO: Mennyire szabad kritizálni az ORSI anyagot?
% TODO: Amúgy lehet csatornák csatornájáról beszélni? :thinking:
% We encountered several problems when we started introducing data channels, which are a fundamental type of variable in the original material.
% DataChannel eq needs quotient, history does not matter -> ownEq
